#!/usr/bin/env python3
"""
GBD 2021 DISEASE-SPECIFIC PAPER RETRIEVAL AND ORGANIZATION (1975-2024) - ALL PAPERS
HANDLES PUBMED 9,999 LIMIT WITH EFFICIENT PROGRESSIVE CHUNKING

SAVE AS: PYTHON/01-00-gbd-disease-retrieval.py
RUN AS: python PYTHON/01-00-gbd-disease-retrieval.py

This script takes the comprehensive GBD 2021 diseases list as starting point,
retrieves ALL papers for each disease efficiently, and organizes them systematically.
Extended analysis period: 1975-2024 (50 years of biomedical research)

EFFICIENT ALL-PAPERS STRATEGY:
- Gets ALL papers for each disease (no compromises)
- Uses efficient progressive chunking: 5-year ‚Üí 1-year ‚Üí monthly
- Much faster than recursive splitting approach
- Handles PubMed 9,999 limit intelligently
- Maximum coverage with reasonable processing time
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import json
from datetime import datetime
import warnings
import logging
import re
import sqlite3
import multiprocessing as mp
from functools import partial
import time
import psutil
import contextlib
from threading import Lock
import gc
from dataclasses import dataclass
import traceback
import xml.etree.ElementTree as ET
from Bio import Entrez
import calendar

# Try to import optional dependencies
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("üìù Note: Install tqdm for better progress bars: pip install tqdm")

try:
    from tabulate import tabulate
    TABULATE_AVAILABLE = True
except ImportError:
    TABULATE_AVAILABLE = False
    print("üìù Note: Install tabulate for better tables: pip install tabulate")

warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class RetrievalConfig:
    """Configuration settings for retrieval"""
    DB_CHUNK_SIZE: int = 50000
    CHECKPOINT_INTERVAL: int = 5  # Save progress every 5 diseases
    MIN_YEAR: int = 1975  # Extended range: 1975-2024 (50 years of research)
    MAX_YEAR: int = 2024
    BATCH_SIZE: int = 200  # PubMed API batch size
    PUBMED_LIMIT: int = 9999  # PubMed's maximum records per search
    # SMART CHUNKING STRATEGY - much more efficient
    INITIAL_CHUNK_YEARS: int = 10  # Start with 10-year chunks
    MIN_CHUNK_YEARS: int = 1      # Minimum chunk size

# Global config
CONFIG = RetrievalConfig()

# Set up paths following user's directory hierarchy
SCRIPT_NAME = "01-00-GBD-DISEASE-RETRIEVAL"
DATA_DIR = "./DATA"
GBD_DISEASES_DIR = os.path.join(DATA_DIR, "GBD-DISEASES")
INDIVIDUAL_DISEASES_DIR = os.path.join(GBD_DISEASES_DIR, "INDIVIDUAL-DISEASES")
ANALYSIS_DIR = f"./ANALYSIS/{SCRIPT_NAME}"
GBD_DATA_FILE = "IHME_GBD_2021_DATA075d3ae61.csv"
PROGRESS_FILE = os.path.join(ANALYSIS_DIR, "retrieval_progress.json")

# Create directories (all in capitals as per user's standards)
os.makedirs(GBD_DISEASES_DIR, exist_ok=True)
os.makedirs(INDIVIDUAL_DISEASES_DIR, exist_ok=True)
os.makedirs(ANALYSIS_DIR, exist_ok=True)

# Configure Entrez for PubMed
Entrez.email = "research.gap.discovery@example.com"
Entrez.tool = "GBDDiseaseRetrieval"

# Configure matplotlib
plt.rcParams.update({
    'font.size': 11,
    'axes.titlesize': 13,
    'axes.labelsize': 11,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.titlesize': 15,
    'font.family': 'sans-serif',
    'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.grid': True,
    'grid.alpha': 0.3,
    'axes.linewidth': 0.8,
    'grid.linewidth': 0.5
})

class GBDDiseaseRetriever:
    """Main class for GBD disease paper retrieval and organization"""
    
    def __init__(self):
        self.diseases_processed = 0
        self.total_papers_retrieved = 0
        self.retrieval_stats = {}
        self.progress_data = {}
        
        logger.info("üè• GBD 2021 DISEASE-SPECIFIC PAPER RETRIEVAL SYSTEM")
        logger.info("=" * 70)
        logger.info(f"üìÇ Data directory: {DATA_DIR}")
        logger.info(f"üìÇ Output directory: {GBD_DISEASES_DIR}")
        logger.info(f"üìÇ Individual diseases: {INDIVIDUAL_DISEASES_DIR}")
        logger.info(f"üìä Analysis directory: {ANALYSIS_DIR}")
        logger.info(f"üìÖ Year range: {CONFIG.MIN_YEAR}-{CONFIG.MAX_YEAR} (50 years of research)")
        logger.info(f"üéØ STRATEGY: Get ALL papers using efficient progressive chunking")
        logger.info(f"‚ö° Approach: 5-year chunks ‚Üí years ‚Üí months (as needed)")
        logger.info("Following directory hierarchy: Scripts in PYTHON/, run from root, output in ANALYSIS/01-00-*")
    
    def create_disease_search_query(self, disease_name, mesh_terms, year=None, month=None, day_range=None):
        """Create search query for specific disease - FIXED VERSION using 00-00 strategy"""
        base_criteria = [
            '"journal article"[Publication Type]',  # Peer-reviewed articles only
            'english[Language]',                     # English language  
            'humans[MeSH Terms]'                     # Human studies only
        ]
        
        # Add date constraints (same as 00-00 script)
        if day_range:
            start_date, end_date = day_range
            date_constraint = f'"{start_date}"[Date - Publication] : "{end_date}"[Date - Publication]'
        elif year and month:
            date_constraint = f'"{year}/{month:02d}"[Date - Publication]'
        elif year:
            date_constraint = f'"{year}"[Date - Publication]'
        else:
            date_constraint = f'"{CONFIG.MIN_YEAR}"[Date - Publication] : "{CONFIG.MAX_YEAR}"[Date - Publication]'
        
        base_criteria.append(date_constraint)
        
        # Add disease-specific search terms
        disease_terms = []
        for term in mesh_terms:
            # Use both MeSH and text word searches for better coverage
            disease_terms.append(f'"{term}"[MeSH Terms] OR "{term}"[Title] OR "{term}"[Abstract]')
        
        if disease_terms:
            disease_query = f"({' OR '.join(disease_terms)})"
            base_criteria.append(disease_query)
        
        query = ' AND '.join(base_criteria)
        return query
    
    def load_gbd_2021_diseases(self):
        """Load and process GBD 2021 diseases as authoritative starting point"""
        logger.info("üìã LOADING GBD 2021 DISEASES AS STARTING POINT...")
        
        gbd_file_path = os.path.join(DATA_DIR, GBD_DATA_FILE)
        
        if not os.path.exists(gbd_file_path):
            logger.error(f"‚ùå GBD data file not found: {gbd_file_path}")
            logger.error(f"Please ensure {GBD_DATA_FILE} is in the {DATA_DIR} directory")
            sys.exit(1)
        
        try:
            gbd_df = pd.read_csv(gbd_file_path)
            logger.info(f"‚úÖ GBD data loaded: {len(gbd_df):,} records")
        except Exception as e:
            logger.error(f"‚ùå Error loading GBD data: {e}")
            sys.exit(1)
        
        # Process GBD data to extract diseases
        disease_df = self.process_gbd_data_for_diseases(gbd_df)
        
        logger.info(f"‚úÖ Processed {len(disease_df)} unique diseases from GBD 2021")
        logger.info(f"üìä Disease categories: {len(disease_df['category'].unique())}")
        
        return disease_df
    
    def process_gbd_data_for_diseases(self, gbd_df):
        """Extract and process disease list from GBD data"""
        logger.info("üîÑ PROCESSING GBD DATA TO EXTRACT DISEASE LIST...")
        
        # Identify column mappings
        potential_mappings = {
            'cause': ['cause_name', 'cause', 'disease_name', 'disease'],
            'measure': ['measure_name', 'measure', 'metric'],
            'value': ['val', 'value', 'metric_value'],
            'year': ['year_id', 'year']
        }
        
        column_mapping = {}
        for key, possible_names in potential_mappings.items():
            for col_name in possible_names:
                if col_name in gbd_df.columns:
                    column_mapping[key] = col_name
                    break
        
        logger.info(f"üìã Column mapping: {column_mapping}")
        
        if 'cause' not in column_mapping:
            logger.error("‚ùå Could not find disease/cause column in GBD data")
            sys.exit(1)
        
        cause_column = column_mapping['cause']
        
        # Get unique diseases
        unique_diseases = gbd_df[cause_column].dropna().unique()
        logger.info(f"üìä Found {len(unique_diseases)} unique diseases in GBD data")
        
        # Create disease dataframe with metadata
        diseases_data = []
        for disease_name in unique_diseases:
            # Skip obviously invalid entries
            if pd.isna(disease_name) or len(str(disease_name).strip()) < 3:
                continue
            
            disease_name = str(disease_name).strip()
            
            # Get disease burden data if available
            disease_records = gbd_df[gbd_df[cause_column] == disease_name]
            
            # Calculate total burden if value column exists
            total_burden = 0
            if 'value' in column_mapping and column_mapping['value'] in disease_records.columns:
                burden_values = pd.to_numeric(disease_records[column_mapping['value']], errors='coerce')
                total_burden = burden_values.sum()
            
            diseases_data.append({
                'disease_name': disease_name,
                'category': self.categorize_disease(disease_name),
                'mesh_terms': self.generate_mesh_terms(disease_name),
                'total_burden': total_burden,
                'gbd_records': len(disease_records),
                'priority_level': self.assign_priority_level(disease_name, total_burden)
            })
        
        disease_df = pd.DataFrame(diseases_data)
        
        # Sort by total burden (highest first)
        disease_df = disease_df.sort_values('total_burden', ascending=False)
        
        logger.info(f"‚úÖ Created disease database with {len(disease_df)} diseases")
        logger.info(f"üìä Categories: {disease_df['category'].value_counts().to_dict()}")
        
        return disease_df
    
    def categorize_disease(self, disease_name):
        """Categorize disease based on name patterns"""
        disease_lower = disease_name.lower()
        
        categories = {
            'Infectious Diseases': [
                'infection', 'viral', 'bacterial', 'parasitic', 'fungal',
                'tuberculosis', 'malaria', 'hiv', 'aids', 'hepatitis',
                'pneumonia', 'diarrhea', 'meningitis', 'sepsis'
            ],
            'Neoplasms': [
                'cancer', 'neoplasm', 'tumor', 'carcinoma', 'lymphoma',
                'leukemia', 'melanoma', 'sarcoma'
            ],
            'Cardiovascular Diseases': [
                'heart', 'cardiac', 'cardiovascular', 'stroke', 'hypertension',
                'ischemic', 'coronary', 'myocardial', 'cerebrovascular'
            ],
            'Mental Disorders': [
                'depression', 'anxiety', 'bipolar', 'schizophrenia',
                'mental', 'psychiatric', 'autism', 'adhd'
            ],
            'Neurological Disorders': [
                'alzheimer', 'parkinson', 'epilepsy', 'dementia',
                'neurological', 'migraine', 'headache', 'sclerosis'
            ],
            'Respiratory Diseases': [
                'respiratory', 'asthma', 'copd', 'lung', 'pulmonary'
            ],
            'Digestive Diseases': [
                'digestive', 'gastro', 'liver', 'cirrhosis', 'ulcer'
            ],
            'Maternal and Child Health': [
                'maternal', 'neonatal', 'birth', 'pregnancy', 'infant'
            ],
            'Nutritional Deficiencies': [
                'malnutrition', 'deficiency', 'anemia', 'vitamin'
            ],
            'Injuries': [
                'injury', 'accident', 'violence', 'suicide', 'poisoning'
            ]
        }
        
        for category, keywords in categories.items():
            for keyword in keywords:
                if keyword in disease_lower:
                    return category
        
        return 'Other Diseases'
    
    def generate_mesh_terms(self, disease_name):
        """Generate search terms for disease (using exact phrase matching)"""
        # Enhanced disease-specific search terms based on validation results
        disease_mapping = {
            'Ischemic heart disease': [
                'Myocardial Ischemia', 'Coronary Artery Disease', 'Coronary Heart Disease'
            ],
            'Stroke': [
                'Cerebrovascular Accident', 'Brain Infarction', 'Cerebral Infarction'
            ],
            'Diabetes mellitus': [
                'Diabetes Mellitus Type 2', 'Diabetes Mellitus Type 1', 'Diabetes Mellitus'
            ],
            'Alzheimer disease and other dementias': [
                'Alzheimer Disease', 'Dementia Alzheimer Type', 'Alzheimer Dementia'
            ],
            'Lung cancer': [
                'Lung Neoplasms', 'Pulmonary Neoplasms', 'Bronchogenic Carcinoma'
            ],
            'Breast cancer': [
                'Breast Neoplasms', 'Mammary Neoplasms', 'Ductal Carcinoma Breast'
            ],
            'HIV/AIDS': [
                'HIV Infections', 'Acquired Immunodeficiency Syndrome', 'HIV-1'
            ],
            'Tuberculosis': [
                'Tuberculosis', 'Tuberculosis Pulmonary', 'Mycobacterium tuberculosis'
            ],
            'Motor neuron disease': [
                'Amyotrophic Lateral Sclerosis', 'Motor Neuron Disease', 'Lou Gehrig Disease'
            ]
        }
        
        # Return specific terms if available, otherwise use disease name
        if disease_name in disease_mapping:
            return disease_mapping[disease_name]
        else:
            # Use the disease name as exact search term
            return [disease_name]
    
    def assign_priority_level(self, disease_name, total_burden):
        """Assign priority level based on disease characteristics"""
        if total_burden > 1000000:
            return 'Critical'
        elif total_burden > 100000:
            return 'High'
        elif total_burden > 10000:
            return 'Moderate'
        else:
            return 'Low'
    
    def search_disease_papers(self, disease_name, mesh_terms):
        """Search for papers related to specific disease - EFFICIENT ALL PAPERS STRATEGY"""
        logger.info(f"üîç Searching papers for: {disease_name}")
        
        try:
            # Create base search query
            search_query = self.create_disease_search_query(disease_name, mesh_terms)
            
            # Get total count first
            handle = Entrez.esearch(db="pubmed", term=search_query, retmax=0)
            search_results = Entrez.read(handle)
            handle.close()
            
            total_count = int(search_results["Count"])
            logger.info(f"   Found {total_count:,} papers for {disease_name}")
            
            if total_count == 0:
                return []
            
            # Check if we need to chunk due to PubMed limit
            if total_count <= CONFIG.PUBMED_LIMIT:
                # Simple case - get all papers directly
                logger.info(f"   üì• Getting all {total_count:,} papers directly")
                return self.retrieve_papers_direct(search_query, disease_name)
            else:
                # Use EFFICIENT PROGRESSIVE CHUNKING to get ALL papers
                logger.info(f"   üéØ Efficient chunking: getting ALL {total_count:,} papers")
                return self.retrieve_all_papers_efficiently(disease_name, mesh_terms, total_count)
            
        except Exception as e:
            logger.error(f"   ‚ùå Error searching {disease_name}: {e}")
            return []
    
    def retrieve_all_papers_efficiently(self, disease_name, mesh_terms, total_expected):
        """Efficiently retrieve ALL papers using progressive chunking"""
        logger.info(f"   üöÄ Starting efficient progressive chunking...")
        
        all_papers = []
        
        # Start with 5-year chunks for efficiency
        start_year = CONFIG.MIN_YEAR
        chunk_size = 5
        
        while start_year <= CONFIG.MAX_YEAR:
            end_year = min(start_year + chunk_size - 1, CONFIG.MAX_YEAR)
            
            # Get papers for this time period
            chunk_papers = self.get_papers_for_period(disease_name, mesh_terms, start_year, end_year)
            all_papers.extend(chunk_papers)
            
            logger.info(f"   üìÖ {start_year}-{end_year}: +{len(chunk_papers):,} papers | Total: {len(all_papers):,}")
            
            start_year = end_year + 1
        
        # Remove duplicates based on PMID
        unique_papers = self.remove_duplicate_papers(all_papers)
        
        if len(all_papers) != len(unique_papers):
            logger.info(f"   üîÑ Removed {len(all_papers) - len(unique_papers):,} duplicates")
        
        logger.info(f"   ‚úÖ Retrieved {len(unique_papers):,} papers (target was {total_expected:,})")
        return unique_papers
    
    def get_papers_for_period(self, disease_name, mesh_terms, start_year, end_year):
        """Get papers for a time period, chunking further if needed"""
        try:
            # Create query for this period
            if start_year == end_year:
                date_constraint = f'"{start_year}"[Date - Publication]'
            else:
                date_constraint = f'"{start_year}"[Date - Publication] : "{end_year}"[Date - Publication]'
            
            base_criteria = [
                '"journal article"[Publication Type]',
                'english[Language]',
                'humans[MeSH Terms]',
                date_constraint
            ]
            
            # Add disease-specific terms
            disease_terms = []
            for term in mesh_terms:
                disease_terms.append(f'"{term}"[MeSH Terms] OR "{term}"[Title] OR "{term}"[Abstract]')
            
            if disease_terms:
                disease_query = f"({' OR '.join(disease_terms)})"
                base_criteria.append(disease_query)
            
            period_query = ' AND '.join(base_criteria)
            
            # Get count for this period
            handle = Entrez.esearch(db="pubmed", term=period_query, retmax=0)
            search_results = Entrez.read(handle)
            handle.close()
            
            period_count = int(search_results["Count"])
            
            if period_count == 0:
                return []
            
            if period_count <= CONFIG.PUBMED_LIMIT:
                # Period fits in one request - get all papers
                handle = Entrez.esearch(db="pubmed", term=period_query, 
                                      retmax=period_count, sort="relevance")
                search_results = Entrez.read(handle)
                handle.close()
                
                pmids = search_results["IdList"]
                return self.fetch_paper_details(pmids, disease_name)
            else:
                # Period too large - break it down further
                if start_year == end_year:
                    # Single year still too large - break into months
                    return self.get_papers_by_months(disease_name, mesh_terms, start_year)
                else:
                    # Multi-year period - break into individual years
                    all_year_papers = []
                    for year in range(start_year, end_year + 1):
                        year_papers = self.get_papers_for_period(disease_name, mesh_terms, year, year)
                        all_year_papers.extend(year_papers)
                    return all_year_papers
                
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è Error getting papers for {start_year}-{end_year}: {e}")
            return []
    
    def get_papers_by_months(self, disease_name, mesh_terms, year):
        """Get papers for a year by breaking it into months"""
        logger.info(f"     üìä Year {year} too large - chunking by months...")
        
        year_papers = []
        
        for month in range(1, 13):
            try:
                month_query = self.create_disease_search_query(disease_name, mesh_terms, year=year, month=month)
                
                # Get count for this month
                handle = Entrez.esearch(db="pubmed", term=month_query, retmax=0)
                search_results = Entrez.read(handle)
                handle.close()
                
                month_count = int(search_results["Count"])
                
                if month_count == 0:
                    continue
                
                if month_count <= CONFIG.PUBMED_LIMIT:
                    # Month fits - get all papers
                    handle = Entrez.esearch(db="pubmed", term=month_query, 
                                          retmax=month_count, sort="relevance")
                    search_results = Entrez.read(handle)
                    handle.close()
                    
                    pmids = search_results["IdList"]
                    month_papers = self.fetch_paper_details(pmids, disease_name)
                    year_papers.extend(month_papers)
                else:
                    # Even month too large - get as many as possible
                    logger.warning(f"     ‚ö†Ô∏è {year}/{month:02d} has {month_count:,} papers - getting 9,999 most relevant")
                    handle = Entrez.esearch(db="pubmed", term=month_query, 
                                          retmax=CONFIG.PUBMED_LIMIT, sort="relevance")
                    search_results = Entrez.read(handle)
                    handle.close()
                    
                    pmids = search_results["IdList"]
                    month_papers = self.fetch_paper_details(pmids, disease_name)
                    year_papers.extend(month_papers)
                
                time.sleep(0.34)  # Rate limiting
                
            except Exception as e:
                logger.warning(f"     ‚ö†Ô∏è Error getting papers for {year}/{month:02d}: {e}")
                continue
        
        return year_papers
    
    # Removed complex chunking methods - using simple direct retrieval only
    
    def retrieve_papers_direct(self, search_query, disease_name):
        """Retrieve papers directly - SIMPLE VERSION"""
        try:
            # Get paper IDs (sorted by relevance for quality)
            handle = Entrez.esearch(db="pubmed", term=search_query, 
                                  retmax=CONFIG.PUBMED_LIMIT, sort="relevance")
            search_results = Entrez.read(handle)
            handle.close()
            
            pmids = search_results["IdList"]
            
            if not pmids:
                return []
            
            # Fetch paper details
            papers = self.fetch_paper_details(pmids, disease_name)
            return papers
            
        except Exception as e:
            logger.error(f"   ‚ùå Error in direct retrieval for {disease_name}: {e}")
            return []
    
    # Legacy chunking methods removed - using simple direct retrieval only
    
    def retrieve_papers_chunked(self, disease_name, mesh_terms):
        """Legacy method - not used in simple approach"""
        logger.warning("Legacy chunking method called - using simple direct retrieval instead")
        return []
    
    def remove_duplicate_papers(self, papers):
        """Remove duplicate papers based on PMID"""
        if not papers:
            return papers
        
        seen_pmids = set()
        unique_papers = []
        
        for paper in papers:
            pmid = paper.get('PMID', '')
            if pmid and pmid not in seen_pmids:
                seen_pmids.add(pmid)
                unique_papers.append(paper)
        
        return unique_papers
    
    # Old year/month/week chunking methods removed for simplicity
    
    def fetch_paper_details(self, pmids, disease_name):
        """Fetch detailed paper information from PubMed"""
        papers = []
        
        # Process in batches
        for i in range(0, len(pmids), CONFIG.BATCH_SIZE):
            batch_pmids = pmids[i:i+CONFIG.BATCH_SIZE]
            
            try:
                handle = Entrez.efetch(db="pubmed", id=batch_pmids, 
                                     rettype="xml", retmode="xml")
                records = handle.read()
                handle.close()
                
                root = ET.fromstring(records)
                
                for article in root.findall('.//PubmedArticle'):
                    paper_data = self.parse_paper_xml(article, disease_name)
                    if paper_data:
                        papers.append(paper_data)
                
                time.sleep(0.34)  # Standard rate limiting
                
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Error fetching batch for {disease_name}: {e}")
                continue
        
        return papers
    
    def parse_paper_xml(self, article_xml, disease_name):
        """Parse individual paper XML to extract fields - SAME AS 00-00 SCRIPT"""
        try:
            paper_data = {}
            
            # Disease name (new field)
            paper_data['Disease_Name'] = disease_name
            
            # PMID
            pmid_elem = article_xml.find('.//PMID')
            paper_data['PMID'] = pmid_elem.text if pmid_elem is not None else ''
            
            # Title
            title_elem = article_xml.find('.//ArticleTitle')
            paper_data['Title'] = title_elem.text if title_elem is not None else ''
            
            # Journal
            journal_elem = article_xml.find('.//Journal/Title')
            if journal_elem is None:
                journal_elem = article_xml.find('.//Journal/ISOAbbreviation')
            paper_data['Journal'] = journal_elem.text if journal_elem is not None else ''
            
            # Publication Year
            year_elem = article_xml.find('.//PubDate/Year')
            if year_elem is None:
                year_elem = article_xml.find('.//PubDate/MedlineDate')
                if year_elem is not None and year_elem.text:
                    year_match = re.search(r'(\d{4})', year_elem.text)
                    paper_data['Year'] = year_match.group(1) if year_match else ''
                else:
                    paper_data['Year'] = ''
            else:
                paper_data['Year'] = year_elem.text
            
            # MeSH Terms
            mesh_terms = []
            for mesh in article_xml.findall('.//MeshHeading/DescriptorName'):
                if mesh.text:
                    mesh_terms.append(mesh.text)
            paper_data['MeSH_Terms'] = '; '.join(mesh_terms)
            
            # Authors and Affiliations
            authors = []
            all_affiliations = set()
            first_author_affiliation = ''
            
            author_list = article_xml.find('.//AuthorList')
            if author_list is not None:
                for i, author in enumerate(author_list.findall('Author')):
                    # Get author name
                    last_name = author.find('LastName')
                    first_name = author.find('ForeName')
                    initials = author.find('Initials')
                    
                    if last_name is not None:
                        author_name = last_name.text
                        if first_name is not None:
                            author_name += f", {first_name.text}"
                        elif initials is not None:
                            author_name += f", {initials.text}"
                        authors.append(author_name)
                    
                    # Get affiliations for this author
                    affiliations = author.findall('.//Affiliation')
                    for aff in affiliations:
                        if aff.text:
                            aff_text = aff.text.strip()
                            all_affiliations.add(aff_text)
                            # Store first author's first affiliation
                            if i == 0 and not first_author_affiliation:
                                first_author_affiliation = aff_text
            
            paper_data['Authors'] = '; '.join(authors)
            paper_data['FirstAuthorAffiliation'] = first_author_affiliation
            paper_data['AllAffiliations'] = '; '.join(sorted(all_affiliations))
            
            return paper_data
            
        except Exception as e:
            logger.warning(f"Error parsing article: {e}")
            return None
    
    def save_disease_papers(self, disease_name, papers):
        """Save papers for individual disease"""
        if not papers:
            return
        
        # Create safe filename
        safe_disease_name = re.sub(r'[^\w\s-]', '', disease_name).strip()
        safe_disease_name = re.sub(r'[-\s]+', '_', safe_disease_name)
        
        disease_file = os.path.join(INDIVIDUAL_DISEASES_DIR, f"{safe_disease_name}.csv")
        
        try:
            df = pd.DataFrame(papers)
            df.to_csv(disease_file, index=False)
            logger.info(f"   üíæ Saved {len(papers)} papers to {disease_file}")
        except Exception as e:
            logger.error(f"   ‚ùå Error saving {disease_name}: {e}")
    
    def load_progress(self):
        """Load retrieval progress if exists - CURRENTLY SET TO START FRESH"""
        # MODIFIED: Starting fresh to ensure all 175 diseases are processed
        # The previous run had stale progress data showing 175 diseases complete
        # but only 3 CSV files actually existed
        
        logger.info("üîÑ Starting fresh retrieval (ignoring any existing progress)")
        logger.info("üí° To resume from existing progress later, modify load_progress() method")
        self.progress_data = {}
        
        # TO RESUME FROM EXISTING PROGRESS: uncomment the lines below
        # try:
        #     if os.path.exists(PROGRESS_FILE):
        #         with open(PROGRESS_FILE, 'r') as f:
        #             self.progress_data = json.load(f)
        #         logger.info(f"üìÇ Loaded progress: {len(self.progress_data)} diseases completed")
        # except Exception as e:
        #     logger.warning(f"Could not load progress: {e}")
        #     self.progress_data = {}
    
    def save_progress(self, disease_name, paper_count):
        """Save retrieval progress"""
        self.progress_data[disease_name] = {
            'papers_retrieved': paper_count,
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            with open(PROGRESS_FILE, 'w') as f:
                json.dump(self.progress_data, f, indent=2)
        except Exception as e:
            logger.warning(f"Could not save progress: {e}")
    
    def retrieve_all_disease_papers(self, disease_df):
        """Retrieve papers for all diseases with progress tracking"""
        logger.info("üöÄ STARTING DISEASE PAPER RETRIEVAL...")
        logger.info(f"üìä Total diseases to process: {len(disease_df)}")
        
        self.load_progress()
        
        # Filter to diseases not yet processed
        processed_diseases = set(self.progress_data.keys())
        remaining_diseases = disease_df[~disease_df['disease_name'].isin(processed_diseases)]
        
        logger.info(f"üìã Remaining diseases: {len(remaining_diseases)}")
        
        if len(remaining_diseases) == 0:
            logger.info("‚úÖ All diseases already processed!")
            return
        
        start_time = time.time()
        
        for i, (_, disease) in enumerate(remaining_diseases.iterrows()):
            disease_name = disease['disease_name']
            mesh_terms = disease['mesh_terms']
            
            logger.info(f"\nüìã [{i+1}/{len(remaining_diseases)}] Processing: {disease_name}")
            logger.info(f"   Category: {disease['category']}")
            logger.info(f"   Priority: {disease['priority_level']}")
            logger.info(f"   Search terms: {mesh_terms}")
            
            # Retrieve papers
            papers = self.search_disease_papers(disease_name, mesh_terms)
            
            if papers:
                # Save individual disease file
                self.save_disease_papers(disease_name, papers)
                
                # Update statistics
                self.total_papers_retrieved += len(papers)
                self.retrieval_stats[disease_name] = len(papers)
            
            # Save progress
            self.save_progress(disease_name, len(papers))
            self.diseases_processed += 1
            
            # Progress update
            elapsed = time.time() - start_time
            if elapsed > 0:
                rate = (i + 1) / elapsed * 60  # diseases per minute
                eta = (len(remaining_diseases) - i - 1) / rate if rate > 0 else 0
                logger.info(f"   üìä Progress: {i+1}/{len(remaining_diseases)} ({rate:.1f}/min, ETA: {eta:.1f}min)")
            
            # Checkpoint save
            if (i + 1) % CONFIG.CHECKPOINT_INTERVAL == 0:
                logger.info(f"   ‚úÖ Checkpoint: {i+1} diseases processed, {self.total_papers_retrieved:,} papers retrieved")
        
        elapsed_time = time.time() - start_time
        logger.info(f"\n‚úÖ RETRIEVAL COMPLETE!")
        logger.info(f"   ‚è±Ô∏è Total time: {elapsed_time/60:.1f} minutes")
        logger.info(f"   üìä Diseases processed: {self.diseases_processed}")
        logger.info(f"   üìÑ Total papers retrieved: {self.total_papers_retrieved:,}")
    
    def create_master_file(self):
        """Combine all disease files into master file"""
        logger.info("üìã CREATING MASTER COMBINED FILE...")
        
        all_papers = []
        disease_files = []
        
        # Find all disease CSV files
        for filename in os.listdir(INDIVIDUAL_DISEASES_DIR):
            if filename.endswith('.csv'):
                disease_files.append(filename)
        
        logger.info(f"üìÇ Found {len(disease_files)} disease files")
        
        # Load and combine
        for filename in disease_files:
            filepath = os.path.join(INDIVIDUAL_DISEASES_DIR, filename)
            try:
                df = pd.read_csv(filepath)
                all_papers.append(df)
                logger.info(f"   ‚úÖ Loaded {len(df)} papers from {filename}")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Error loading {filename}: {e}")
        
        if not all_papers:
            logger.error("‚ùå No papers to combine")
            return None
        
        # Combine all papers
        master_df = pd.concat(all_papers, ignore_index=True)
        
        # Remove duplicates based on PMID
        initial_count = len(master_df)
        master_df = master_df.drop_duplicates(subset=['PMID'], keep='first')
        final_count = len(master_df)
        
        logger.info(f"üìä Combined papers: {final_count:,} unique papers")
        if initial_count != final_count:
            logger.info(f"   Removed {initial_count - final_count:,} duplicates")
        
        # Save master file
        master_file = os.path.join(GBD_DISEASES_DIR, "master_gbd_diseases_papers.csv")
        master_df.to_csv(master_file, index=False)
        
        logger.info(f"‚úÖ Master file saved: {master_file}")
        
        return master_df
    
    def calculate_comprehensive_statistics(self, master_df=None):
        """Calculate comprehensive statistics - FIXED VERSION with proper string handling"""
        logger.info("üìä CALCULATING COMPREHENSIVE STATISTICS...")
        
        if master_df is None:
            master_file = os.path.join(GBD_DISEASES_DIR, "master_gbd_diseases_papers.csv")
            if os.path.exists(master_file):
                master_df = pd.read_csv(master_file)
            else:
                logger.error("‚ùå No master file found for statistics")
                return None
        
        stats = {}
        
        # Basic statistics
        stats['total_papers'] = len(master_df)
        stats['unique_diseases'] = master_df['Disease_Name'].nunique()
        stats['unique_pmids'] = master_df['PMID'].nunique()
        
        # Year analysis
        if 'Year' in master_df.columns:
            years = pd.to_numeric(master_df['Year'], errors='coerce')
            valid_years = years.dropna()
            
            stats['year_range'] = (int(valid_years.min()), int(valid_years.max())) if len(valid_years) > 0 else (None, None)
            stats['papers_with_valid_year'] = len(valid_years)
            stats['year_distribution'] = valid_years.value_counts().sort_index().to_dict()
        
        # Disease distribution
        disease_counts = master_df['Disease_Name'].value_counts()
        stats['papers_per_disease'] = disease_counts.to_dict()
        stats['avg_papers_per_disease'] = disease_counts.mean()
        stats['median_papers_per_disease'] = disease_counts.median()
        
        # Content analysis - FIXED to handle non-string values properly
        def safe_string_check(series):
            """Safely check if string column has content"""
            try:
                # Convert to string first, then check length
                return (series.astype(str).str.len() > 0).sum()
            except:
                # Fallback: check for non-null, non-empty values
                return series.notna().sum()
        
        stats['papers_with_mesh'] = safe_string_check(master_df['MeSH_Terms'])
        stats['papers_with_title'] = safe_string_check(master_df['Title'])
        stats['papers_with_authors'] = safe_string_check(master_df['Authors'])
        stats['papers_with_affiliations'] = safe_string_check(master_df['FirstAuthorAffiliation'])
        
        # Journal analysis
        if 'Journal' in master_df.columns:
            journal_counts = master_df['Journal'].value_counts()
            stats['unique_journals'] = len(journal_counts)
            stats['top_journals'] = journal_counts.head(20).to_dict()
        
        # MeSH terms analysis
        if 'MeSH_Terms' in master_df.columns:
            all_mesh_terms = []
            for mesh_text in master_df['MeSH_Terms'].dropna():
                if mesh_text and str(mesh_text).strip():
                    terms = [term.strip() for term in str(mesh_text).split(';')]
                    all_mesh_terms.extend(terms)
            
            mesh_counter = Counter(all_mesh_terms)
            stats['unique_mesh_terms'] = len(mesh_counter)
            stats['top_mesh_terms'] = dict(mesh_counter.most_common(50))
        
        # Recent papers analysis (last 5 years: 2020-2024)
        if 'Year' in master_df.columns:
            recent_years = valid_years[valid_years >= (CONFIG.MAX_YEAR - 4)]  # 2020-2024
            stats['recent_papers_5yr'] = len(recent_years)
            stats['recent_percentage'] = len(recent_years) / len(valid_years) * 100 if len(valid_years) > 0 else 0
        
        # Data quality metrics - FIXED version
        stats['data_completeness'] = {
            'pmid': safe_string_check(master_df['PMID']) / len(master_df) * 100,
            'title': safe_string_check(master_df['Title']) / len(master_df) * 100,
            'year': stats['papers_with_valid_year'] / len(master_df) * 100,
            'mesh': stats['papers_with_mesh'] / len(master_df) * 100,
            'authors': stats['papers_with_authors'] / len(master_df) * 100
        }
        
        self.retrieval_stats = stats
        return stats
    
    def create_comprehensive_visualizations(self, stats):
        """Create comprehensive visualizations like the original analyzer"""
        logger.info("üìä CREATING COMPREHENSIVE VISUALIZATIONS...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('GBD Diseases Complete Paper Retrieval - ALL Papers (1975-2024)', 
                    fontsize=16, fontweight='bold')
        
        # 1. Papers per disease distribution
        ax1 = axes[0, 0]
        disease_counts = list(stats['papers_per_disease'].values())
        ax1.hist(disease_counts, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
        ax1.set_xlabel('Papers per Disease')
        ax1.set_ylabel('Number of Diseases')
        ax1.set_title('Distribution of Papers per Disease')
        ax1.set_yscale('log')
        
        # 2. Year distribution
        ax2 = axes[0, 1]
        if 'year_distribution' in stats:
            years = sorted(stats['year_distribution'].keys())
            counts = [stats['year_distribution'][year] for year in years]
            ax2.plot(years, counts, marker='o', linewidth=2)
            ax2.set_xlabel('Year')
            ax2.set_ylabel('Number of Papers')
            ax2.set_title(f'Papers by Year ({min(years)}-{max(years)}) - 50 Year Span')
            ax2.grid(True, alpha=0.3)
        
        # 3. Top diseases by paper count
        ax3 = axes[0, 2]
        top_diseases = dict(sorted(stats['papers_per_disease'].items(), 
                                 key=lambda x: x[1], reverse=True)[:15])
        disease_names = [name[:30] + '...' if len(name) > 30 else name 
                        for name in top_diseases.keys()]
        counts = list(top_diseases.values())
        
        y_pos = range(len(disease_names))
        ax3.barh(y_pos, counts, color='darkgreen', alpha=0.7)
        ax3.set_yticks(y_pos)
        ax3.set_yticklabels(disease_names, fontsize=8)
        ax3.set_xlabel('Number of Papers')
        ax3.set_title('Top 15 Diseases by Paper Count')
        ax3.invert_yaxis()
        
        # 4. Data completeness
        ax4 = axes[1, 0]
        completeness = stats['data_completeness']
        fields = list(completeness.keys())
        percentages = list(completeness.values())
        
        bars = ax4.bar(fields, percentages, color='orange', alpha=0.7)
        ax4.set_ylabel('Completeness (%)')
        ax4.set_title('Data Field Completeness')
        ax4.set_ylim(0, 100)
        
        # Add percentage labels on bars
        for bar, pct in zip(bars, percentages):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{pct:.1f}%', ha='center', va='bottom')
        
        # 5. Top journals
        ax5 = axes[1, 1]
        if 'top_journals' in stats:
            top_journals = dict(list(stats['top_journals'].items())[:10])
            journal_names = [name[:30] + '...' if len(name) > 30 else name 
                           for name in top_journals.keys()]
            journal_counts = list(top_journals.values())
            
            y_pos = range(len(journal_names))
            ax5.barh(y_pos, journal_counts, color='purple', alpha=0.7)
            ax5.set_yticks(y_pos)
            ax5.set_yticklabels(journal_names, fontsize=8)
            ax5.set_xlabel('Number of Papers')
            ax5.set_title('Top 10 Journals')
            ax5.invert_yaxis()
        
        # 6. Summary statistics
        ax6 = axes[1, 2]
        ax6.axis('off')
        
        summary_text = f"""
COMPLETE RETRIEVAL SUMMARY (1975-2024)
ALL PAPERS - NO LIMITS

Total Papers: {stats['total_papers']:,}
Unique Diseases: {stats['unique_diseases']:,}
Unique PMIDs: {stats['unique_pmids']:,}

Year Range: {stats['year_range'][0]}-{stats['year_range'][1]} (50 years)
Recent Papers (2020-2024): {stats['recent_papers_5yr']:,} ({stats['recent_percentage']:.1f}%)

Avg Papers/Disease: {stats['avg_papers_per_disease']:.1f}
Median Papers/Disease: {stats['median_papers_per_disease']:.1f}

Papers with MeSH: {stats['papers_with_mesh']:,}
Unique Journals: {stats['unique_journals']:,}
Unique MeSH Terms: {stats['unique_mesh_terms']:,}

Methodology: Complete retrieval
bypassing PubMed 9,999 limit

Data Quality:
PMID: {stats['data_completeness']['pmid']:.1f}%
Title: {stats['data_completeness']['title']:.1f}%
Year: {stats['data_completeness']['year']:.1f}%
MeSH: {stats['data_completeness']['mesh']:.1f}%
Authors: {stats['data_completeness']['authors']:.1f}%
        """
        
        ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, fontsize=10,
                verticalalignment='top', 
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        
        # Save visualization
        viz_file = os.path.join(ANALYSIS_DIR, 'gbd_disease_retrieval_statistics.png')
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        logger.info(f"‚úÖ Visualizations saved: {viz_file}")
        
        plt.show()
    
    def export_comprehensive_results(self, stats):
        """Export comprehensive results and statistics"""
        logger.info("üíæ EXPORTING COMPREHENSIVE RESULTS...")
        
        # Export detailed statistics
        stats_file = os.path.join(ANALYSIS_DIR, 'retrieval_statistics.json')
        with open(stats_file, 'w') as f:
            # Convert numpy types for JSON serialization
            stats_serializable = self.convert_for_json(stats)
            json.dump(stats_serializable, f, indent=2)
        
        # Export disease summary
        if 'papers_per_disease' in stats:
            disease_summary = []
            for disease, count in stats['papers_per_disease'].items():
                disease_summary.append({
                    'Disease_Name': disease,
                    'Papers_Retrieved': count,
                    'Percentage_of_Total': count / stats['total_papers'] * 100
                })
            
            disease_df = pd.DataFrame(disease_summary)
            disease_df = disease_df.sort_values('Papers_Retrieved', ascending=False)
            
            disease_file = os.path.join(ANALYSIS_DIR, 'disease_retrieval_summary.csv')
            disease_df.to_csv(disease_file, index=False)
            logger.info(f"‚úÖ Disease summary saved: {disease_file}")
        
        # Export year analysis
        if 'year_distribution' in stats:
            year_data = []
            for year, count in sorted(stats['year_distribution'].items()):
                year_data.append({
                    'Year': year,
                    'Papers': count,
                    'Percentage': count / stats['total_papers'] * 100
                })
            
            year_df = pd.DataFrame(year_data)
            year_file = os.path.join(ANALYSIS_DIR, 'year_distribution.csv')
            year_df.to_csv(year_file, index=False)
            logger.info(f"‚úÖ Year analysis saved: {year_file}")
        
        # Export journal analysis
        if 'top_journals' in stats:
            journal_data = []
            for journal, count in stats['top_journals'].items():
                journal_data.append({
                    'Journal': journal,
                    'Papers': count,
                    'Percentage': count / stats['total_papers'] * 100
                })
            
            journal_df = pd.DataFrame(journal_data)
            journal_file = os.path.join(ANALYSIS_DIR, 'journal_analysis.csv')
            journal_df.to_csv(journal_file, index=False)
            logger.info(f"‚úÖ Journal analysis saved: {journal_file}")
        
        # Create comprehensive report
        self.create_comprehensive_report(stats)
        
        logger.info(f"‚úÖ All results exported to: {ANALYSIS_DIR}")
    
    def convert_for_json(self, obj):
        """Convert numpy types for JSON serialization"""
        if isinstance(obj, dict):
            return {key: self.convert_for_json(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_for_json(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        else:
            return obj
    
    def create_comprehensive_report(self, stats):
        """Create comprehensive text report"""
        report_file = os.path.join(ANALYSIS_DIR, 'gbd_disease_retrieval_report.txt')
        
        with open(report_file, 'w') as f:
            f.write("GBD 2021 DISEASE-SPECIFIC PAPER RETRIEVAL REPORT - ALL PAPERS\n")
            f.write("=" * 60 + "\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Retrieval period: {CONFIG.MIN_YEAR}-{CONFIG.MAX_YEAR} (50 years)\n")
            f.write(f"Strategy: Efficient progressive chunking - ALL papers retrieved\n\n")
            
            f.write("RETRIEVAL SUMMARY:\n")
            f.write("-" * 30 + "\n")
            f.write(f"Total papers retrieved: {stats['total_papers']:,}\n")
            f.write(f"Unique diseases: {stats['unique_diseases']:,}\n")
            f.write(f"Unique PMIDs: {stats['unique_pmids']:,}\n")
            f.write(f"Year range: {stats['year_range'][0]}-{stats['year_range'][1]}\n")
            f.write(f"Average papers per disease: {stats['avg_papers_per_disease']:.1f}\n")
            f.write(f"Median papers per disease: {stats['median_papers_per_disease']:.1f}\n\n")
            
            f.write("DATA QUALITY METRICS:\n")
            f.write("-" * 30 + "\n")
            for field, completeness in stats['data_completeness'].items():
                f.write(f"{field.upper()}: {completeness:.1f}% complete\n")
            f.write("\n")
            
            f.write("TOP 20 DISEASES BY PAPER COUNT:\n")
            f.write("-" * 40 + "\n")
            top_diseases = sorted(stats['papers_per_disease'].items(), 
                                key=lambda x: x[1], reverse=True)[:20]
            for i, (disease, count) in enumerate(top_diseases, 1):
                percentage = count / stats['total_papers'] * 100
                f.write(f"{i:2d}. {disease}: {count:,} papers ({percentage:.2f}%)\n")
            f.write("\n")
            
            f.write("TOP 10 JOURNALS:\n")
            f.write("-" * 20 + "\n")
            if 'top_journals' in stats:
                for i, (journal, count) in enumerate(list(stats['top_journals'].items())[:10], 1):
                    percentage = count / stats['total_papers'] * 100
                    f.write(f"{i:2d}. {journal}: {count:,} papers ({percentage:.2f}%)\n")
            f.write("\n")
            
            f.write("METHODOLOGY:\n")
            f.write("-" * 15 + "\n")
            f.write("1. Started with comprehensive GBD 2021 diseases as authoritative list\n")
            f.write("2. Used exact phrase matching for disease-specific searches\n")
            f.write("3. Extended analysis period: 1975-2024 (50 years of research)\n")
            f.write("4. Efficient progressive chunking: 5-year ‚Üí 1-year ‚Üí monthly\n")
            f.write("5. ALL papers retrieved for each disease (no limits)\n")
            f.write("6. Retrieved papers with full metadata (PMID, Title, Journal, etc.)\n")
            f.write("7. Organized into individual disease files and master combined file\n")
            f.write("8. Applied data quality validation and comprehensive statistics\n\n")
            
            f.write("OUTPUT FILES:\n")
            f.write("-" * 15 + "\n")
            f.write("‚Ä¢ Individual disease files: DATA/GBD-DISEASES/INDIVIDUAL-DISEASES/*.csv\n")
            f.write("‚Ä¢ Master combined file: DATA/GBD-DISEASES/master_gbd_diseases_papers.csv\n")
            f.write("‚Ä¢ Statistics and analysis: ANALYSIS/01-00-GBD-DISEASE-RETRIEVAL/\n")
        
        logger.info(f"‚úÖ Comprehensive report saved: {report_file}")
    
    def run_complete_retrieval(self):
        """Run the complete retrieval pipeline"""
        logger.info("üöÄ STARTING COMPLETE GBD DISEASE RETRIEVAL PIPELINE...")
        start_time = time.time()
        
        try:
            # Step 1: Load GBD diseases
            disease_df = self.load_gbd_2021_diseases()
            
            # Step 2: Retrieve papers for all diseases
            self.retrieve_all_disease_papers(disease_df)
            
            # Step 3: Create master combined file
            master_df = self.create_master_file()
            
            # Step 4: Calculate comprehensive statistics
            stats = self.calculate_comprehensive_statistics(master_df)
            
            # Step 5: Skip visualizations for now (user will do later)
            logger.info("‚è≠Ô∏è Skipping visualizations (will be done later)")
            
            # Step 6: Export comprehensive results
            if stats:
                self.export_comprehensive_results(stats)
            
            total_time = time.time() - start_time
            
            logger.info("\n‚úÖ COMPLETE RETRIEVAL PIPELINE FINISHED!")
            logger.info(f"‚è±Ô∏è Total time: {total_time/60:.1f} minutes")
            logger.info(f"üìä Diseases processed: {self.diseases_processed}")
            logger.info(f"üìÑ Papers retrieved: {self.total_papers_retrieved:,}")
            
            if stats:
                logger.info(f"üìã Final statistics:")
                logger.info(f"   ‚Ä¢ Total unique papers: {stats['total_papers']:,}")
                logger.info(f"   ‚Ä¢ Unique diseases: {stats['unique_diseases']:,}")
                logger.info(f"   ‚Ä¢ Year range: {stats['year_range'][0]}-{stats['year_range'][1]} (50 years)")
                logger.info(f"   ‚Ä¢ Recent papers (2020-2024): {stats['recent_papers_5yr']:,}")
                logger.info(f"   ‚Ä¢ Data completeness: {stats['data_completeness']['title']:.1f}% titles")
            
            logger.info(f"\nüìÇ OUTPUT STRUCTURE:")
            logger.info(f"   üìÅ Individual diseases: {INDIVIDUAL_DISEASES_DIR}")
            logger.info(f"   üìÑ Master file: {os.path.join(GBD_DISEASES_DIR, 'master_gbd_diseases_papers.csv')}")
            logger.info(f"   üìä Analysis results: {ANALYSIS_DIR}")
            logger.info(f"   üìú Script: PYTHON/01-00-gbd-disease-retrieval.py")
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            traceback.print_exc()
            sys.exit(1)

def main():
    """Main execution function"""
    print("üè• GBD 2021 DISEASE-SPECIFIC PAPER RETRIEVAL SYSTEM - ALL PAPERS")
    print("=" * 70)
    print("üìã Starting with comprehensive GBD 2021 diseases as authoritative list")
    print("üéØ GOAL: Retrieve ALL papers for each disease (no compromises)")
    print("üìÖ Extended analysis period: 1975-2024 (50 years of research)")
    print("üìÇ Organizing into structured dataset for future analysis")
    print("üîÑ STARTING FRESH - Processing all 175 diseases")
    print("‚è≠Ô∏è Skipping visualizations (will be done in separate analysis)")
    print()
    print("üìÅ DIRECTORY HIERARCHY:")
    print("   üìú Script location: PYTHON/01-00-gbd-disease-retrieval.py")
    print("   üè† Run from: . (root directory)")
    print("   üìÇ Data files: DATA/")
    print("   üìä Output: ANALYSIS/01-00-GBD-DISEASE-RETRIEVAL/")
    print()
    print("üéØ EFFICIENT ALL-PAPERS STRATEGY:")
    print("   üìä Get ALL papers for each disease (no limits)")
    print("   üöÄ Start with efficient 5-year chunks")
    print("   üîÑ Break down to years, then months if needed")
    print("   ‚ö° Much faster than recursive splitting")
    print("   üéØ Maximum coverage with reasonable speed")
    print()
    
    # Check required directories and files
    if not os.path.exists(DATA_DIR):
        print(f"‚ùå DATA directory not found: {DATA_DIR}")
        print("Make sure you're running from the root directory")
        sys.exit(1)
    
    gbd_file = os.path.join(DATA_DIR, GBD_DATA_FILE)
    if not os.path.exists(gbd_file):
        print(f"‚ùå GBD data file not found: {gbd_file}")
        print(f"Please ensure {GBD_DATA_FILE} is in the DATA directory")
        sys.exit(1)
    
    try:
        # Create retriever and run complete pipeline
        retriever = GBDDiseaseRetriever()
        retriever.run_complete_retrieval()
        
        print("\nüéØ RETRIEVAL SYSTEM COMPLETE!")
        print("üìÅ Ready for comprehensive analysis with next script")
        print(f"üìÇ All data organized in: {GBD_DISEASES_DIR}")
        print(f"üìä Statistics available in: {ANALYSIS_DIR}")
        print()
        print("üöÄ KEY ACHIEVEMENTS:")
        print("   ‚úÖ ALL PAPERS: Complete retrieval for each disease")
        print("   ‚úÖ Efficient progressive chunking strategy")
        print("   ‚úÖ 50 years of comprehensive biomedical research coverage")
        print("   ‚úÖ All 175 GBD diseases systematically processed")
        print("   ‚è≠Ô∏è Visualizations skipped (for separate analysis step)")
        print("   üéØ Maximum paper coverage achieved")
        print()
        print("üìÅ OUTPUT STRUCTURE CREATED:")
        print(f"   üìÇ {INDIVIDUAL_DISEASES_DIR}")
        print(f"   üìÑ {os.path.join(GBD_DISEASES_DIR, 'master_gbd_diseases_papers.csv')}")
        print(f"   üìä {ANALYSIS_DIR}")
        print("   üìú Save script as: PYTHON/01-00-gbd-disease-retrieval.py")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Retrieval interrupted by user")
        print("Progress has been saved and can be resumed")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()